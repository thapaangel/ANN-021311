{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFDCxN3gG8N8h+q+6BHzv+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thapaangel/ANN-021311/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbSs59vwWTb-",
        "outputId": "8f06b5b3-0c3b-48fc-aff9-6b76dd81f7f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perceptron for AND Gate:\n",
            "Epoch 1\n",
            "Weights: [0.1 0.1], Bias: 0.0\n",
            "Epoch 2\n",
            "Weights: [0.2 0.1], Bias: -0.1\n",
            "Epoch 3\n",
            "Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Epoch 4\n",
            "Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Converged.\n",
            "\n",
            "Final Accuracy: 100.00%\n",
            "\n",
            "\n",
            "Training Perceptron for OR Gate:\n",
            "Epoch 1\n",
            "Weights: [0.  0.1], Bias: 0.0\n",
            "Epoch 2\n",
            "Weights: [0.1 0.1], Bias: 0.0\n",
            "Epoch 3\n",
            "Weights: [0.1 0.1], Bias: -0.1\n",
            "Epoch 4\n",
            "Weights: [0.1 0.1], Bias: -0.1\n",
            "Converged.\n",
            "\n",
            "Final Accuracy: 100.00%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Perceptron for 2 input basic gates\n",
        "import numpy as np\n",
        "class Perceptron:\n",
        "    def __init__(self, input_size, learning_rate=0.1, max_epochs=100):\n",
        "        self.weights = np.zeros(input_size)\n",
        "        self.bias = 0\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = max_epochs\n",
        "\n",
        "    def step_activation(self, x):\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def predict(self, x):\n",
        "        z = np.dot(self.weights, x) + self.bias\n",
        "        return self.step_activation(z)\n",
        "\n",
        "    def train(self, X, Y):\n",
        "        for epoch in range(self.epochs):\n",
        "            total_error = 0\n",
        "            print(f\"Epoch {epoch+1}\")\n",
        "            for x, y in zip(X, Y):\n",
        "                y_pred = self.predict(x)\n",
        "                error = y - y_pred\n",
        "                self.weights += self.lr * error * x\n",
        "                self.bias += self.lr * error\n",
        "                total_error += abs(error)\n",
        "            print(f\"Weights: {self.weights}, Bias: {self.bias}\")\n",
        "            if total_error == 0:\n",
        "                print(\"Converged.\")\n",
        "                break\n",
        "        correct = sum([self.predict(x) == y for x, y in zip(X, Y)])\n",
        "        accuracy = correct / len(Y)\n",
        "        print(f\"\\nFinal Accuracy: {accuracy*100:.2f}%\\n\")\n",
        "\n",
        "# Training for AND Gate\n",
        "print(\"Training Perceptron for AND Gate:\")\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "Y_AND = np.array([0, 0, 0, 1])\n",
        "p_and = Perceptron(input_size=2)\n",
        "p_and.train(X, Y_AND)\n",
        "\n",
        "# Training for OR Gate\n",
        "print(\"\\nTraining Perceptron for OR Gate:\")\n",
        "Y_OR = np.array([0, 1, 1, 1])\n",
        "p_or = Perceptron(input_size=2)\n",
        "p_or.train(X, Y_OR)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Perceptron for n input basic gates and or\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "class PerceptronNInput:\n",
        "    def __init__(self, n_inputs, learning_rate=0.1, max_epochs=100):\n",
        "        self.weights = np.zeros(n_inputs)\n",
        "        self.bias = 0\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = max_epochs\n",
        "\n",
        "    def step_activation(self, x):\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.step_activation(np.dot(self.weights, x) + self.bias)\n",
        "\n",
        "    def train(self, X, Y):\n",
        "        for epoch in range(self.epochs):\n",
        "            total_error = 0\n",
        "            for x, y in zip(X, Y):\n",
        "                y_pred = self.predict(x)\n",
        "                error = y - y_pred\n",
        "                self.weights += self.lr * error * x\n",
        "                self.bias += self.lr * error\n",
        "                total_error += abs(error)\n",
        "            if total_error == 0:\n",
        "                break\n",
        "\n",
        "        accuracy = sum(self.predict(x) == y for x, y in zip(X, Y)) / len(Y)\n",
        "        return self.weights, self.bias, accuracy\n",
        "\n",
        "def generate_truth_table(n):\n",
        "    inputs = list(product([0, 1], repeat=n))\n",
        "    X = np.array(inputs)\n",
        "    Y_and = np.array([int(all(x)) for x in X])\n",
        "    Y_or = np.array([int(any(x)) for x in X])\n",
        "    return X, Y_and, Y_or\n",
        "\n",
        "def test_perceptron_n_input(n):\n",
        "    print(f\"\\nTraining {n}-Input AND Gate:\")\n",
        "    X, Y_and, Y_or = generate_truth_table(n)\n",
        "    p_and = PerceptronNInput(n_inputs=n)\n",
        "    w_and, b_and, acc_and = p_and.train(X, Y_and)\n",
        "    print(f\"Weights: {w_and}, Bias: {b_and}, Accuracy: {acc_and*100:.2f}%\")\n",
        "\n",
        "    print(f\"\\nTraining {n}-Input OR Gate:\")\n",
        "    p_or = PerceptronNInput(n_inputs=n)\n",
        "    w_or, b_or, acc_or = p_or.train(X, Y_or)\n",
        "    print(f\"Weights: {w_or}, Bias: {b_or}, Accuracy: {acc_or*100:.2f}%\")\n",
        "\n",
        "# Test with n = 3 and n = 4\n",
        "test_perceptron_n_input(3)\n",
        "test_perceptron_n_input(4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ78avMwXI6o",
        "outputId": "ee8ff0df-1df7-499e-9b01-b2bfc8c1c1d7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training 3-Input AND Gate:\n",
            "Weights: [0.1 0.1 0.1], Bias: -0.20000000000000004, Accuracy: 100.00%\n",
            "\n",
            "Training 3-Input OR Gate:\n",
            "Weights: [0.1 0.1 0.1], Bias: -0.1, Accuracy: 100.00%\n",
            "\n",
            "Training 4-Input AND Gate:\n",
            "Weights: [0.4 0.2 0.1 0.1], Bias: -0.7999999999999999, Accuracy: 100.00%\n",
            "\n",
            "Training 4-Input OR Gate:\n",
            "Weights: [0.1 0.1 0.1 0.1], Bias: -0.1, Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Perceptron for linear function with 3 features\n",
        "import numpy as np\n",
        "\n",
        "class LinearPerceptron:\n",
        "    def __init__(self, input_size, learning_rate=0.01, max_epochs=100):\n",
        "        self.weights = np.zeros(input_size)\n",
        "        self.bias = 0\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = max_epochs\n",
        "\n",
        "    def predict(self, x):\n",
        "        return np.dot(self.weights, x) + self.bias  # linear output\n",
        "\n",
        "    def train(self, X, Y):\n",
        "        for epoch in range(self.epochs):\n",
        "            total_error = 0\n",
        "            for x, y in zip(X, Y):\n",
        "                y_pred = self.predict(x)\n",
        "                error = y - y_pred\n",
        "                self.weights += self.lr * error * x\n",
        "                self.bias += self.lr * error\n",
        "                total_error += error ** 2\n",
        "            mse = total_error / len(X)\n",
        "            print(f\"Epoch {epoch+1}: MSE = {mse:.4f}\")# mean square error\n",
        "        print(f\"\\nFinal Weights: {self.weights}, Bias: {self.bias}\")\n",
        "\n",
        "# Generate dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(10, 3)  # 10 samples, x1, x2, x3 in [0, 1]\n",
        "true_weights = np.array([2, 3, -1])\n",
        "true_bias = 5\n",
        "Y = np.dot(X, true_weights) + true_bias\n",
        "\n",
        "# Train the model\n",
        "model = LinearPerceptron(input_size=3)\n",
        "model.train(X, Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbGiBhcKX5md",
        "outputId": "dfa58ae2-936f-4906-e7f9-3be64d6e6240"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: MSE = 40.6459\n",
            "Epoch 2: MSE = 29.6449\n",
            "Epoch 3: MSE = 21.6726\n",
            "Epoch 4: MSE = 15.8948\n",
            "Epoch 5: MSE = 11.7071\n",
            "Epoch 6: MSE = 8.6713\n",
            "Epoch 7: MSE = 6.4701\n",
            "Epoch 8: MSE = 4.8734\n",
            "Epoch 9: MSE = 3.7147\n",
            "Epoch 10: MSE = 2.8732\n",
            "Epoch 11: MSE = 2.2615\n",
            "Epoch 12: MSE = 1.8161\n",
            "Epoch 13: MSE = 1.4912\n",
            "Epoch 14: MSE = 1.2535\n",
            "Epoch 15: MSE = 1.0790\n",
            "Epoch 16: MSE = 0.9503\n",
            "Epoch 17: MSE = 0.8546\n",
            "Epoch 18: MSE = 0.7830\n",
            "Epoch 19: MSE = 0.7287\n",
            "Epoch 20: MSE = 0.6869\n",
            "Epoch 21: MSE = 0.6543\n",
            "Epoch 22: MSE = 0.6284\n",
            "Epoch 23: MSE = 0.6072\n",
            "Epoch 24: MSE = 0.5895\n",
            "Epoch 25: MSE = 0.5744\n",
            "Epoch 26: MSE = 0.5612\n",
            "Epoch 27: MSE = 0.5494\n",
            "Epoch 28: MSE = 0.5386\n",
            "Epoch 29: MSE = 0.5285\n",
            "Epoch 30: MSE = 0.5191\n",
            "Epoch 31: MSE = 0.5101\n",
            "Epoch 32: MSE = 0.5014\n",
            "Epoch 33: MSE = 0.4931\n",
            "Epoch 34: MSE = 0.4850\n",
            "Epoch 35: MSE = 0.4771\n",
            "Epoch 36: MSE = 0.4693\n",
            "Epoch 37: MSE = 0.4618\n",
            "Epoch 38: MSE = 0.4544\n",
            "Epoch 39: MSE = 0.4471\n",
            "Epoch 40: MSE = 0.4399\n",
            "Epoch 41: MSE = 0.4329\n",
            "Epoch 42: MSE = 0.4260\n",
            "Epoch 43: MSE = 0.4192\n",
            "Epoch 44: MSE = 0.4125\n",
            "Epoch 45: MSE = 0.4059\n",
            "Epoch 46: MSE = 0.3994\n",
            "Epoch 47: MSE = 0.3930\n",
            "Epoch 48: MSE = 0.3868\n",
            "Epoch 49: MSE = 0.3806\n",
            "Epoch 50: MSE = 0.3745\n",
            "Epoch 51: MSE = 0.3686\n",
            "Epoch 52: MSE = 0.3627\n",
            "Epoch 53: MSE = 0.3569\n",
            "Epoch 54: MSE = 0.3512\n",
            "Epoch 55: MSE = 0.3456\n",
            "Epoch 56: MSE = 0.3401\n",
            "Epoch 57: MSE = 0.3347\n",
            "Epoch 58: MSE = 0.3294\n",
            "Epoch 59: MSE = 0.3241\n",
            "Epoch 60: MSE = 0.3190\n",
            "Epoch 61: MSE = 0.3139\n",
            "Epoch 62: MSE = 0.3089\n",
            "Epoch 63: MSE = 0.3040\n",
            "Epoch 64: MSE = 0.2992\n",
            "Epoch 65: MSE = 0.2944\n",
            "Epoch 66: MSE = 0.2898\n",
            "Epoch 67: MSE = 0.2852\n",
            "Epoch 68: MSE = 0.2806\n",
            "Epoch 69: MSE = 0.2762\n",
            "Epoch 70: MSE = 0.2718\n",
            "Epoch 71: MSE = 0.2675\n",
            "Epoch 72: MSE = 0.2633\n",
            "Epoch 73: MSE = 0.2591\n",
            "Epoch 74: MSE = 0.2550\n",
            "Epoch 75: MSE = 0.2510\n",
            "Epoch 76: MSE = 0.2470\n",
            "Epoch 77: MSE = 0.2431\n",
            "Epoch 78: MSE = 0.2393\n",
            "Epoch 79: MSE = 0.2355\n",
            "Epoch 80: MSE = 0.2318\n",
            "Epoch 81: MSE = 0.2281\n",
            "Epoch 82: MSE = 0.2245\n",
            "Epoch 83: MSE = 0.2210\n",
            "Epoch 84: MSE = 0.2175\n",
            "Epoch 85: MSE = 0.2140\n",
            "Epoch 86: MSE = 0.2107\n",
            "Epoch 87: MSE = 0.2074\n",
            "Epoch 88: MSE = 0.2041\n",
            "Epoch 89: MSE = 0.2009\n",
            "Epoch 90: MSE = 0.1977\n",
            "Epoch 91: MSE = 0.1946\n",
            "Epoch 92: MSE = 0.1916\n",
            "Epoch 93: MSE = 0.1886\n",
            "Epoch 94: MSE = 0.1856\n",
            "Epoch 95: MSE = 0.1827\n",
            "Epoch 96: MSE = 0.1798\n",
            "Epoch 97: MSE = 0.1770\n",
            "Epoch 98: MSE = 0.1742\n",
            "Epoch 99: MSE = 0.1715\n",
            "Epoch 100: MSE = 0.1688\n",
            "\n",
            "Final Weights: [2.02103307 2.70937935 0.39012764], Bias: 4.4982401004553445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Perceptron for linear function with n features\n",
        "import numpy as np\n",
        "\n",
        "class LinearPerceptronN:\n",
        "    def __init__(self, n_features, learning_rate=0.01, max_epochs=100):\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = max_epochs\n",
        "\n",
        "    def predict(self, x):\n",
        "        return np.dot(self.weights, x) + self.bias\n",
        "\n",
        "    def train(self, X, Y):\n",
        "        for epoch in range(self.epochs):\n",
        "            total_error = 0\n",
        "            for x, y in zip(X, Y):\n",
        "                y_pred = self.predict(x)\n",
        "                error = y - y_pred\n",
        "                self.weights += self.lr * error * x\n",
        "                self.bias += self.lr * error\n",
        "                total_error += error ** 2\n",
        "            mse = total_error / len(X)\n",
        "            print(f\"Epoch {epoch+1}: MSE = {mse:.4f}\")\n",
        "        print(f\"\\nFinal Weights: {self.weights}, Bias: {self.bias}\")\n",
        "\n",
        "# Set number of features\n",
        "n = 5  # You can change this to any number of features\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(10, n)  # 10 samples, n features\n",
        "true_weights = np.array([i+1 for i in range(n)])  # [1, 2, 3, ..., n]\n",
        "true_bias = 4\n",
        "Y = np.dot(X, true_weights) + true_bias\n",
        "\n",
        "# Train model\n",
        "model = LinearPerceptronN(n_features=n)\n",
        "model.train(X, Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5qkCa9xYxxZ",
        "outputId": "f72b4c99-9f3f-40f1-d49e-d397026a0aa5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: MSE = 99.2107\n",
            "Epoch 2: MSE = 66.0038\n",
            "Epoch 3: MSE = 44.0067\n",
            "Epoch 4: MSE = 29.4329\n",
            "Epoch 5: MSE = 19.7749\n",
            "Epoch 6: MSE = 13.3726\n",
            "Epoch 7: MSE = 9.1265\n",
            "Epoch 8: MSE = 6.3086\n",
            "Epoch 9: MSE = 4.4368\n",
            "Epoch 10: MSE = 3.1919\n",
            "Epoch 11: MSE = 2.3623\n",
            "Epoch 12: MSE = 1.8081\n",
            "Epoch 13: MSE = 1.4365\n",
            "Epoch 14: MSE = 1.1860\n",
            "Epoch 15: MSE = 1.0159\n",
            "Epoch 16: MSE = 0.8991\n",
            "Epoch 17: MSE = 0.8179\n",
            "Epoch 18: MSE = 0.7603\n",
            "Epoch 19: MSE = 0.7185\n",
            "Epoch 20: MSE = 0.6872\n",
            "Epoch 21: MSE = 0.6630\n",
            "Epoch 22: MSE = 0.6437\n",
            "Epoch 23: MSE = 0.6275\n",
            "Epoch 24: MSE = 0.6136\n",
            "Epoch 25: MSE = 0.6013\n",
            "Epoch 26: MSE = 0.5901\n",
            "Epoch 27: MSE = 0.5796\n",
            "Epoch 28: MSE = 0.5697\n",
            "Epoch 29: MSE = 0.5603\n",
            "Epoch 30: MSE = 0.5513\n",
            "Epoch 31: MSE = 0.5425\n",
            "Epoch 32: MSE = 0.5340\n",
            "Epoch 33: MSE = 0.5257\n",
            "Epoch 34: MSE = 0.5176\n",
            "Epoch 35: MSE = 0.5096\n",
            "Epoch 36: MSE = 0.5018\n",
            "Epoch 37: MSE = 0.4942\n",
            "Epoch 38: MSE = 0.4867\n",
            "Epoch 39: MSE = 0.4794\n",
            "Epoch 40: MSE = 0.4722\n",
            "Epoch 41: MSE = 0.4651\n",
            "Epoch 42: MSE = 0.4581\n",
            "Epoch 43: MSE = 0.4513\n",
            "Epoch 44: MSE = 0.4446\n",
            "Epoch 45: MSE = 0.4380\n",
            "Epoch 46: MSE = 0.4315\n",
            "Epoch 47: MSE = 0.4251\n",
            "Epoch 48: MSE = 0.4189\n",
            "Epoch 49: MSE = 0.4127\n",
            "Epoch 50: MSE = 0.4067\n",
            "Epoch 51: MSE = 0.4007\n",
            "Epoch 52: MSE = 0.3949\n",
            "Epoch 53: MSE = 0.3891\n",
            "Epoch 54: MSE = 0.3835\n",
            "Epoch 55: MSE = 0.3779\n",
            "Epoch 56: MSE = 0.3725\n",
            "Epoch 57: MSE = 0.3671\n",
            "Epoch 58: MSE = 0.3618\n",
            "Epoch 59: MSE = 0.3566\n",
            "Epoch 60: MSE = 0.3515\n",
            "Epoch 61: MSE = 0.3465\n",
            "Epoch 62: MSE = 0.3415\n",
            "Epoch 63: MSE = 0.3367\n",
            "Epoch 64: MSE = 0.3319\n",
            "Epoch 65: MSE = 0.3272\n",
            "Epoch 66: MSE = 0.3225\n",
            "Epoch 67: MSE = 0.3180\n",
            "Epoch 68: MSE = 0.3135\n",
            "Epoch 69: MSE = 0.3091\n",
            "Epoch 70: MSE = 0.3047\n",
            "Epoch 71: MSE = 0.3005\n",
            "Epoch 72: MSE = 0.2963\n",
            "Epoch 73: MSE = 0.2921\n",
            "Epoch 74: MSE = 0.2881\n",
            "Epoch 75: MSE = 0.2841\n",
            "Epoch 76: MSE = 0.2801\n",
            "Epoch 77: MSE = 0.2762\n",
            "Epoch 78: MSE = 0.2724\n",
            "Epoch 79: MSE = 0.2687\n",
            "Epoch 80: MSE = 0.2650\n",
            "Epoch 81: MSE = 0.2613\n",
            "Epoch 82: MSE = 0.2577\n",
            "Epoch 83: MSE = 0.2542\n",
            "Epoch 84: MSE = 0.2507\n",
            "Epoch 85: MSE = 0.2473\n",
            "Epoch 86: MSE = 0.2439\n",
            "Epoch 87: MSE = 0.2406\n",
            "Epoch 88: MSE = 0.2374\n",
            "Epoch 89: MSE = 0.2342\n",
            "Epoch 90: MSE = 0.2310\n",
            "Epoch 91: MSE = 0.2279\n",
            "Epoch 92: MSE = 0.2248\n",
            "Epoch 93: MSE = 0.2218\n",
            "Epoch 94: MSE = 0.2188\n",
            "Epoch 95: MSE = 0.2159\n",
            "Epoch 96: MSE = 0.2130\n",
            "Epoch 97: MSE = 0.2102\n",
            "Epoch 98: MSE = 0.2074\n",
            "Epoch 99: MSE = 0.2046\n",
            "Epoch 100: MSE = 0.2019\n",
            "\n",
            "Final Weights: [1.43580928 1.65576065 2.42156949 3.48985868 3.42326874], Bias: 5.093453849991454\n"
          ]
        }
      ]
    }
  ]
}